from flask import Flask, jsonify, render_template, request, redirect, url_for
from FireRecognizer import detect_fire_from_image
from SkinCancerRecognizer import detect_skin_cancer
import os
import subprocess

app = Flask(__name__)
UPLOAD_PATH = 'static/uploads/input.jpg'
OUTPUT_PATH = 'static/outputs/output.jpg'

@app.route('/')
def index():
    return render_template('index.html')

@app.route('/run_model', methods=['POST'])
def run_model():
    if 'image' not in request.files:
        return redirect(request.url)

    file = request.files['image']
    if file.filename == '':
        return redirect(request.url)

    # ğŸ‘‡ GÃ¶rseli sabit isimle kaydet
    file.save(UPLOAD_PATH)

    # Modeli Ã§alÄ±ÅŸtÄ±r ve sonucu sabit olarak Ã¼ret
    _, detected = detect_fire_from_image(UPLOAD_PATH)

    return jsonify({
        "input": "uploads/input.jpg",
        "output": "outputs/output.jpg",
        "detected": detected
    })
@app.route('/run_skin_model', methods=['POST'])
def run_skin_model():
    file = request.files['image']
    filename = 'input.jpg'
    input_path = os.path.join('static/uploads', filename)
    file.save(input_path)

    output_path, label = detect_skin_cancer(input_path)

    return jsonify({
        "input": "uploads/input.jpg",
        "output": "outputs/output.jpg",
        "label": label
    })
from PlateRecognizer import detect_plate_from_image

@app.route('/run_plate_model', methods=['POST'])
def run_plate_model():
    file = request.files['image']
    filename = 'input.jpg'
    input_path = os.path.join('static/uploads', filename)
    file.save(input_path)

    output_path, result_text = detect_plate_from_image(input_path)

    return jsonify({
        "input": "uploads/input.jpg",
        "output": "outputs/output_input.jpg",
        "text": result_text
    })
@app.route('/ask_bot', methods=['POST'])
@app.route('/ask_bot', methods=['POST'])
def ask_bot():
    user_input = request.json.get('message', '')

    system_prompt = """
Senin adÄ±n "Z" ve bir yapay zeka asistanÄ±sÄ±n.
Senin gÃ¶revlerin:
Sen ModelZ platformunda Ã§alÄ±ÅŸan bir yapay zeka asistanÄ±sÄ±n.
KullanÄ±cÄ±lara sadece ModelZ'deki yapay zeka modelleri hakkÄ±nda bilgi ver.
KonuÅŸma tarzÄ±n kÄ±sa, net ve teknik bilgilendirici olmalÄ±. Samimi ama cÃ¼mleler kÄ±sa ve doÄŸrudan. Maksimum 2 cÃ¼mle ile cevap ver.
KullanÄ±cÄ±ya yardÄ±mcÄ± olabilmek iÃ§in sorularÄ±nÄ± anlamaya Ã§alÄ±ÅŸ. EÄŸer kullanÄ±cÄ± bir model hakkÄ±nda bilgi isterse, o modelin ne yaptÄ±ÄŸÄ±nÄ± ve nasÄ±l Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ± Ã§ok kÄ±sa maksimum 2 cÃ¼mle ile aÃ§Ä±kla.
EÄŸer kullanÄ±cÄ± baÅŸka konular sorarsa, nazikÃ§e bu konuda yardÄ±mcÄ± olamayacaÄŸÄ±nÄ± belirt.
KullanÄ±cÄ±ya nasÄ±l yardÄ±mcÄ± olabilirim gibi ÅŸÄ±k ve ilgili sorular sor. bunlar Ã§ok kÄ±sa ve net olmalÄ±. EÄŸer kullanÄ±cÄ± isterse model ile bir demo yapabilmeli.

Åu an ModelZ platformunda bulunan modeller:
- YangÄ±n Tespiti (Haar Cascade)
- Cilt Kanseri Tespiti (CNN)
- Plaka TanÄ±ma ve OCR
- Evo AI SaÄŸlÄ±k AsistanÄ± (LLM)
Model dÄ±ÅŸÄ± sorulara cevap verme. Bu modellerin dÄ±ÅŸÄ±nda baÅŸka bir model yok. 

KullanÄ±cÄ±nÄ±n sorusu:
"""


    full_input = system_prompt + user_input

    try:
        result = subprocess.run(
            ['ollama', 'run', 'llama3.1'],
            input=full_input,
            capture_output=True,
            text=True,
            encoding='utf-8',
            timeout=30
        )

        response_text = result.stdout.strip()
        return jsonify({'response': response_text})

    except Exception as e:
        return jsonify({'response': f'Hata oluÅŸtu: {str(e)}'})


'''if __name__ == '__main__':
    port = int(os.environ.get("PORT", 10000))
    app.run(debug=False, host='0.0.0.0', port=port)'''
if __name__ == '__main__':
    app.run(debug=True, port=8501)
